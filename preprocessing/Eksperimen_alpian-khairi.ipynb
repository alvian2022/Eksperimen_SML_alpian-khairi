{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Perkenalan Dataset**\n",
        "\n",
        "Dataset yang digunakan: **Iris Dataset**\n",
        "- Sumber: UCI Machine Learning Repository\n",
        "- Jumlah sampel: 150\n",
        "- Jumlah fitur: 4 (sepal_length, sepal_width, petal_length, petal_width)\n",
        "- Target: 3 kelas (setosa, versicolor, virginica)\n",
        "- Tipe data: Numerik untuk fitur, kategorikal untuk target"
      ],
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Import Library**"
      ],
      "metadata": {
        "id": "fKADPWcFKlj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Utilities\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Memuat Dataset**"
      ],
      "metadata": {
        "id": "f3YIEnAFKrKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['species'] = iris.target\n",
        "\n",
        "# Map target numbers to species names\n",
        "species_mapping = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
        "df['species_name'] = df['species'].map(species_mapping)\n",
        "\n",
        "# Save raw data\n",
        "df.to_csv('iris_raw.csv', index=False)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nDataset info:\")\n",
        "print(df.info())\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "bgZkbJLpK9UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic statistics\n",
        "print(\"=== BASIC STATISTICS ===\")\n",
        "print(df.describe())\n",
        "\n",
        "print(\"\\n=== MISSING VALUES ===\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "print(\"\\n=== TARGET DISTRIBUTION ===\")\n",
        "print(df['species_name'].value_counts())"
      ],
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Distribution of features\n",
        "df[iris.feature_names].hist(bins=20, ax=axes[0,0], alpha=0.7)\n",
        "axes[0,0].set_title('Feature Distributions')\n",
        "\n",
        "# 2. Species distribution\n",
        "df['species_name'].value_counts().plot(kind='bar', ax=axes[0,1], color='skyblue')\n",
        "axes[0,1].set_title('Species Distribution')\n",
        "axes[0,1].set_xlabel('Species')\n",
        "axes[0,1].set_ylabel('Count')\n",
        "\n",
        "# 3. Correlation heatmap\n",
        "correlation_matrix = df[iris.feature_names].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', ax=axes[1,0])\n",
        "axes[1,0].set_title('Feature Correlation Matrix')\n",
        "\n",
        "# 4. Pairplot sample (sepal features)\n",
        "scatter = axes[1,1].scatter(df['sepal length (cm)'], df['sepal width (cm)'], \n",
        "                           c=df['species'], cmap='viridis', alpha=0.7)\n",
        "axes[1,1].set_xlabel('Sepal Length (cm)')\n",
        "axes[1,1].set_ylabel('Sepal Width (cm)')\n",
        "axes[1,1].set_title('Sepal Length vs Width by Species')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDA_plots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More detailed EDA\n",
        "print(\"=== FEATURE STATISTICS BY SPECIES ===\")\n",
        "for species in df['species_name'].unique():\n",
        "    print(f\"\\n{species.upper()}:\")\n",
        "    species_data = df[df['species_name'] == species][iris.feature_names]\n",
        "    print(species_data.describe())"
      ],
      "metadata": {
        "id": "EDA_stats"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plots for each feature by species\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(iris.feature_names):\n",
        "    sns.boxplot(data=df, x='species_name', y=feature, ax=axes[i])\n",
        "    axes[i].set_title(f'{feature} by Species')\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDA_boxplots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Data Preprocessing**"
      ],
      "metadata": {
        "id": "cpgHfgnSK3ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== DATA PREPROCESSING STEPS ===\")\n",
        "\n",
        "# 1. Check for missing values\n",
        "print(\"\\n1. Checking for missing values:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "print(f\"Total missing values: {missing_values.sum()}\")\n",
        "\n",
        "# 2. Check for duplicates\n",
        "print(\"\\n2. Checking for duplicates:\")\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(\"Removing duplicates...\")\n",
        "    df_clean = df.drop_duplicates()\n",
        "    print(f\"Shape after removing duplicates: {df_clean.shape}\")\n",
        "else:\n",
        "    df_clean = df.copy()\n",
        "    print(\"No duplicates found.\")"
      ],
      "metadata": {
        "id": "preprocessing_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Outlier detection using IQR method\n",
        "print(\"\\n3. Outlier Detection using IQR method:\")\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers.index\n",
        "\n",
        "outlier_indices = set()\n",
        "for feature in iris.feature_names:\n",
        "    feature_outliers = detect_outliers_iqr(df_clean, feature)\n",
        "    print(f\"{feature}: {len(feature_outliers)} outliers\")\n",
        "    outlier_indices.update(feature_outliers)\n",
        "\n",
        "print(f\"\\nTotal unique outlier indices: {len(outlier_indices)}\")\n",
        "print(f\"Outlier percentage: {len(outlier_indices)/len(df_clean)*100:.2f}%\")\n",
        "\n",
        "# For this dataset, we'll keep outliers as they might be important for species classification\n",
        "print(\"\\nKeeping outliers as they may be important for species classification.\")"
      ],
      "metadata": {
        "id": "preprocessing_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Feature scaling\n",
        "print(\"\\n4. Feature Scaling:\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df_clean[iris.feature_names].copy()\n",
        "y = df_clean['species'].copy()\n",
        "\n",
        "print(f\"Original feature ranges:\")\n",
        "for feature in iris.feature_names:\n",
        "    print(f\"{feature}: {X[feature].min():.2f} - {X[feature].max():.2f}\")\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled_df = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
        "\n",
        "print(f\"\\nAfter scaling - feature ranges:\")\n",
        "for feature in iris.feature_names:\n",
        "    print(f\"{feature}: {X_scaled_df[feature].min():.2f} - {X_scaled_df[feature].max():.2f}\")"
      ],
      "metadata": {
        "id": "preprocessing_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Target encoding (optional, for consistency)\n",
        "print(\"\\n5. Target Encoding:\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "print(f\"Original target classes: {np.unique(y)}\")\n",
        "print(f\"Encoded target classes: {np.unique(y_encoded)}\")\n",
        "print(f\"Label mapping: {dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))}\")"
      ],
      "metadata": {
        "id": "preprocessing_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Create final preprocessed dataset\n",
        "print(\"\\n6. Creating Final Preprocessed Dataset:\")\n",
        "\n",
        "# Combine scaled features with encoded target\n",
        "processed_df = X_scaled_df.copy()\n",
        "processed_df['target'] = y_encoded\n",
        "\n",
        "print(f\"Final preprocessed dataset shape: {processed_df.shape}\")\n",
        "print(f\"\\nPreprocessed dataset info:\")\n",
        "print(processed_df.info())\n",
        "\n",
        "print(f\"\\nFirst 5 rows of preprocessed data:\")\n",
        "print(processed_df.head())\n",
        "\n",
        "print(f\"\\nTarget distribution in preprocessed data:\")\n",
        "print(processed_df['target'].value_counts().sort_index())"
      ],
      "metadata": {
        "id": "preprocessing_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Save preprocessed data\n",
        "print(\"\\n7. Saving Preprocessed Data:\")\n",
        "\n",
        "# Save to CSV\n",
        "processed_df.to_csv('iris_preprocessing.csv', index=False)\n",
        "print(\"Preprocessed data saved as 'iris_preprocessing.csv'\")\n",
        "\n",
        "# Verification\n",
        "verification_df = pd.read_csv('iris_preprocessing.csv')\n",
        "print(f\"\\nVerification - loaded file shape: {verification_df.shape}\")\n",
        "print(\"Preprocessing completed successfully!\")"
      ],
      "metadata": {
        "id": "preprocessing_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Quick model validation to ensure preprocessing quality\n",
        "print(\"\\n8. Quick Model Validation:\")\n",
        "\n",
        "# Split data for quick validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled_df, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "# Train a simple model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Quick validation accuracy: {accuracy:.4f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "print(\"\\n=== PREPROCESSING SUMMARY ===\")\n",
        "print(f\"✓ Original dataset shape: {df.shape}\")\n",
        "print(f\"✓ Final preprocessed shape: {processed_df.shape}\")\n",
        "print(f\"✓ Missing values handled: {df.isnull().sum().sum()} → 0\")\n",
        "print(f\"✓ Duplicates removed: {duplicates}\")\n",
        "print(f\"✓ Features scaled: ✓\")\n",
        "print(f\"✓ Target encoded: ✓\")\n",
        "print(f\"✓ Quick validation accuracy: {accuracy:.4f}\")\n",
        "print(f\"✓ Data saved successfully: ✓\")"
      ],
      "metadata": {
        "id": "preprocessing_validation"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}