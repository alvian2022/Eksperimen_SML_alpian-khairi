{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Perkenalan Dataset**\n",
        "\n",
        "Dataset yang digunakan: **Diabetes Prediction Dataset**\n",
        "- Sumber: Healthcare/Medical Data\n",
        "- Jumlah sampel: Variable (depends on dataset size)\n",
        "- Jumlah fitur: 8 (gender, age, hypertension, heart_disease, smoking_history, bmi, HbA1c_level, blood_glucose_level)\n",
        "- Target: diabetes (0 = No Diabetes, 1 = Diabetes)\n",
        "- Tipe data: Mixed (numerik untuk sebagian besar fitur, kategorikal untuk gender dan smoking_history)"
      ],
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Import Library**"
      ],
      "metadata": {
        "id": "fKADPWcFKlj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Utilities\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Memuat Dataset**"
      ],
      "metadata": {
        "id": "f3YIEnAFKrKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Diabetes dataset\n",
        "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
        "\n",
        "# Define feature categories\n",
        "categorical_features = ['gender', 'smoking_history']\n",
        "numerical_features = ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
        "target_column = 'diabetes'\n",
        "\n",
        "# Save raw data\n",
        "df.to_csv('diabetes_raw.csv', index=False)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nDataset columns: {list(df.columns)}\")\n",
        "print(f\"\\nDataset info:\")\n",
        "print(df.info())\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "bgZkbJLpK9UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic statistics\n",
        "print(\"=== BASIC STATISTICS ===\")\n",
        "print(\"\\nNumerical Features:\")\n",
        "print(df[numerical_features].describe())\n",
        "\n",
        "print(\"\\n=== MISSING VALUES ===\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "print(f\"Total missing values: {missing_values.sum()}\")\n",
        "\n",
        "print(\"\\n=== TARGET DISTRIBUTION ===\")\n",
        "target_dist = df[target_column].value_counts()\n",
        "print(target_dist)\n",
        "print(f\"Diabetes prevalence: {target_dist[1]/len(df)*100:.2f}%\")\n",
        "\n",
        "print(\"\\n=== CATEGORICAL FEATURES DISTRIBUTION ===\")\n",
        "for col in categorical_features:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(df[col].value_counts())"
      ],
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive visualizations\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# 1. Target distribution\n",
        "target_counts = df[target_column].value_counts()\n",
        "axes[0].pie(target_counts.values, labels=['No Diabetes', 'Diabetes'], autopct='%1.1f%%', \n",
        "           colors=['lightblue', 'lightcoral'])\n",
        "axes[0].set_title('Diabetes Distribution')\n",
        "\n",
        "# 2. Age distribution\n",
        "axes[1].hist(df['age'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[1].set_title('Age Distribution')\n",
        "axes[1].set_xlabel('Age')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "# 3. BMI distribution\n",
        "axes[2].hist(df['bmi'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[2].set_title('BMI Distribution')\n",
        "axes[2].set_xlabel('BMI')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "\n",
        "# 4. Gender distribution\n",
        "gender_counts = df['gender'].value_counts()\n",
        "axes[3].bar(gender_counts.index, gender_counts.values, color=['pink', 'lightblue'])\n",
        "axes[3].set_title('Gender Distribution')\n",
        "axes[3].set_xlabel('Gender')\n",
        "axes[3].set_ylabel('Count')\n",
        "axes[3].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 5. Smoking history distribution\n",
        "smoking_counts = df['smoking_history'].value_counts()\n",
        "axes[4].bar(range(len(smoking_counts)), smoking_counts.values, color='orange', alpha=0.7)\n",
        "axes[4].set_xticks(range(len(smoking_counts)))\n",
        "axes[4].set_xticklabels(smoking_counts.index, rotation=45, ha='right')\n",
        "axes[4].set_title('Smoking History Distribution')\n",
        "axes[4].set_ylabel('Count')\n",
        "\n",
        "# 6. HbA1c levels\n",
        "axes[5].hist(df['HbA1c_level'], bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[5].set_title('HbA1c Level Distribution')\n",
        "axes[5].set_xlabel('HbA1c Level')\n",
        "axes[5].set_ylabel('Frequency')\n",
        "\n",
        "# 7. Blood glucose levels\n",
        "axes[6].hist(df['blood_glucose_level'], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
        "axes[6].set_title('Blood Glucose Level Distribution')\n",
        "axes[6].set_xlabel('Blood Glucose Level')\n",
        "axes[6].set_ylabel('Frequency')\n",
        "\n",
        "# 8. Hypertension vs Diabetes\n",
        "hyp_diabetes = pd.crosstab(df['hypertension'], df[target_column])\n",
        "hyp_diabetes.plot(kind='bar', ax=axes[7], color=['lightblue', 'lightcoral'])\n",
        "axes[7].set_title('Hypertension vs Diabetes')\n",
        "axes[7].set_xlabel('Hypertension (0=No, 1=Yes)')\n",
        "axes[7].set_ylabel('Count')\n",
        "axes[7].legend(['No Diabetes', 'Diabetes'])\n",
        "axes[7].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 9. Heart disease vs Diabetes\n",
        "heart_diabetes = pd.crosstab(df['heart_disease'], df[target_column])\n",
        "heart_diabetes.plot(kind='bar', ax=axes[8], color=['lightgreen', 'orange'])\n",
        "axes[8].set_title('Heart Disease vs Diabetes')\n",
        "axes[8].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
        "axes[8].set_ylabel('Count')\n",
        "axes[8].legend(['No Diabetes', 'Diabetes'])\n",
        "axes[8].tick_params(axis='x', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDA_plots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation analysis\n",
        "plt.figure(figsize=(12, 8))\n",
        "all_numerical = numerical_features + [target_column]\n",
        "correlation_matrix = df[all_numerical].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "           square=True, linewidths=0.5)\n",
        "plt.title('Correlation Matrix - Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDA_correlation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature analysis by diabetes status\n",
        "print(\"=== FEATURE STATISTICS BY DIABETES STATUS ===\")\n",
        "for feature in numerical_features:\n",
        "    print(f\"\\n{feature.upper()}:\")\n",
        "    no_diabetes = df[df[target_column] == 0][feature]\n",
        "    diabetes = df[df[target_column] == 1][feature]\n",
        "    \n",
        "    print(f\"No Diabetes - Mean: {no_diabetes.mean():.2f}, Std: {no_diabetes.std():.2f}\")\n",
        "    print(f\"Diabetes - Mean: {diabetes.mean():.2f}, Std: {diabetes.std():.2f}\")\n",
        "    print(f\"Difference: {diabetes.mean() - no_diabetes.mean():.2f}\")"
      ],
      "metadata": {
        "id": "EDA_stats"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plots for numerical features by diabetes status\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(numerical_features):\n",
        "    sns.boxplot(data=df, x=target_column, y=feature, ax=axes[i])\n",
        "    axes[i].set_title(f'{feature} by Diabetes Status')\n",
        "    axes[i].set_xlabel('Diabetes (0=No, 1=Yes)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDA_boxplots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Data Preprocessing**"
      ],
      "metadata": {
        "id": "cpgHfgnSK3ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== DATA PREPROCESSING STEPS ===\")\n",
        "\n",
        "# 1. Check for missing values\n",
        "print(\"\\n1. Checking for missing values:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "print(f\"Total missing values: {missing_values.sum()}\")\n",
        "\n",
        "# Handle missing values if any\n",
        "df_clean = df.copy()\n",
        "if missing_values.sum() > 0:\n",
        "    print(\"Handling missing values...\")\n",
        "    # Fill numerical missing values with median\n",
        "    for col in numerical_features:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            median_val = df_clean[col].median()\n",
        "            df_clean[col].fillna(median_val, inplace=True)\n",
        "            print(f\"  - Filled {col} with median: {median_val:.2f}\")\n",
        "    \n",
        "    # Fill categorical missing values with mode\n",
        "    for col in categorical_features:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            mode_val = df_clean[col].mode()[0]\n",
        "            df_clean[col].fillna(mode_val, inplace=True)\n",
        "            print(f\"  - Filled {col} with mode: {mode_val}\")\n",
        "else:\n",
        "    print(\"No missing values found.\")\n",
        "\n",
        "# 2. Check for duplicates\n",
        "print(\"\\n2. Checking for duplicates:\")\n",
        "duplicates = df_clean.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(\"Removing duplicates...\")\n",
        "    df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
        "    print(f\"Shape after removing duplicates: {df_clean.shape}\")\n",
        "else:\n",
        "    print(\"No duplicates found.\")"
      ],
      "metadata": {
        "id": "preprocessing_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Data type cleaning and validation\n",
        "print(\"\\n3. Data type cleaning and validation:\")\n",
        "\n",
        "# Clean categorical features\n",
        "for col in categorical_features:\n",
        "    df_clean[col] = df_clean[col].astype(str).str.strip().str.lower()\n",
        "    print(f\"Cleaned {col}: {df_clean[col].unique()}\")\n",
        "\n",
        "# Validate numerical ranges\n",
        "print(\"\\nValidating numerical ranges:\")\n",
        "original_len = len(df_clean)\n",
        "\n",
        "# Age validation (0-120)\n",
        "if 'age' in df_clean.columns:\n",
        "    df_clean = df_clean[(df_clean['age'] > 0) & (df_clean['age'] <= 120)]\n",
        "    print(f\"Age validation: {original_len} → {len(df_clean)} rows\")\n",
        "\n",
        "# BMI validation (10-70)\n",
        "if 'bmi' in df_clean.columns:\n",
        "    df_clean = df_clean[(df_clean['bmi'] > 0) & (df_clean['bmi'] <= 70)]\n",
        "    print(f\"BMI validation: kept rows with BMI > 0 and <= 70\")\n",
        "\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "print(f\"Final shape after validation: {df_clean.shape}\")"
      ],
      "metadata": {
        "id": "preprocessing_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Outlier detection using IQR method\n",
        "print(\"\\n4. Outlier Detection using IQR method:\")\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers.index, lower_bound, upper_bound\n",
        "\n",
        "outlier_summary = {}\n",
        "for feature in numerical_features:\n",
        "    outlier_indices, lower, upper = detect_outliers_iqr(df_clean, feature)\n",
        "    outlier_count = len(outlier_indices)\n",
        "    outlier_percentage = outlier_count/len(df_clean)*100\n",
        "    \n",
        "    outlier_summary[feature] = {\n",
        "        'count': outlier_count,\n",
        "        'percentage': outlier_percentage,\n",
        "        'bounds': (lower, upper)\n",
        "    }\n",
        "    \n",
        "    print(f\"{feature}: {outlier_count} outliers ({outlier_percentage:.2f}%) - Bounds: [{lower:.2f}, {upper:.2f}]\")\n",
        "\n",
        "print(\"\\nNote: Keeping outliers as they may be important for diabetes prediction.\")"
      ],
      "metadata": {
        "id": "preprocessing_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Feature encoding\n",
        "print(\"\\n5. Feature Encoding:\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df_clean.drop(target_column, axis=1)\n",
        "y = df_clean[target_column]\n",
        "\n",
        "# Encode categorical features using Label Encoding\n",
        "label_encoders = {}\n",
        "X_encoded = X.copy()\n",
        "\n",
        "for col in categorical_features:\n",
        "    if col in X_encoded.columns:\n",
        "        label_encoders[col] = LabelEncoder()\n",
        "        X_encoded[col] = label_encoders[col].fit_transform(X_encoded[col])\n",
        "        \n",
        "        # Show encoding mapping\n",
        "        mapping = dict(zip(label_encoders[col].classes_, \n",
        "                          label_encoders[col].transform(label_encoders[col].classes_)))\n",
        "        print(f\"Encoded {col}: {mapping}\")\n",
        "\n",
        "print(f\"\\nFeatures after encoding: {list(X_encoded.columns)}\")"
      ],
      "metadata": {
        "id": "preprocessing_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Feature scaling\n",
        "print(\"\\n6. Feature Scaling:\")\n",
        "\n",
        "print(f\"Original ranges for numerical features:\")\n",
        "for feature in numerical_features:\n",
        "    if feature in X_encoded.columns:\n",
        "        print(f\"  {feature}: {X_encoded[feature].min():.2f} - {X_encoded[feature].max():.2f}\")\n",
        "\n",
        "# Apply StandardScaler to numerical features only\n",
        "scaler = StandardScaler()\n",
        "X_scaled = X_encoded.copy()\n",
        "\n",
        "# Scale only numerical features\n",
        "numerical_cols_to_scale = [col for col in numerical_features if col in X_scaled.columns]\n",
        "X_scaled[numerical_cols_to_scale] = scaler.fit_transform(X_encoded[numerical_cols_to_scale])\n",
        "\n",
        "print(f\"\\nAfter scaling - ranges for numerical features:\")\n",
        "for feature in numerical_cols_to_scale:\n",
        "    print(f\"  {feature}: {X_scaled[feature].min():.2f} - {X_scaled[feature].max():.2f}\")\n",
        "\n",
        "print(f\"\\nCategorical features (not scaled): {[col for col in categorical_features if col in X_scaled.columns]}\")"
      ],
      "metadata": {
        "id": "preprocessing_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Create final preprocessed dataset\n",
        "print(\"\\n7. Creating Final Preprocessed Dataset:\")\n",
        "\n",
        "# Combine scaled features with target\n",
        "processed_df = X_scaled.copy()\n",
        "processed_df[target_column] = y\n",
        "\n",
        "print(f\"Final preprocessed dataset shape: {processed_df.shape}\")\n",
        "print(f\"Features: {list(processed_df.columns[:-1])}\")\n",
        "print(f\"Target column: {target_column}\")\n",
        "\n",
        "print(f\"\\nTarget distribution in preprocessed data:\")\n",
        "target_dist = processed_df[target_column].value_counts().sort_index()\n",
        "print(target_dist)\n",
        "print(f\"Diabetes prevalence: {target_dist[1]/len(processed_df)*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nFirst 5 rows of preprocessed data:\")\n",
        "print(processed_df.head())\n",
        "\n",
        "# Save preprocessed data\n",
        "processed_df.to_csv('diabetes_preprocessed.csv', index=False)\n",
        "print(\"\\nPreprocessed data saved as 'diabetes_preprocessed.csv'\")\n",
        "\n",
        "# Verification\n",
        "verification_df = pd.read_csv('diabetes_preprocessed.csv')\n",
        "print(f\"Verification - loaded file shape: {verification_df.shape}\")"
      ],
      "metadata": {
        "id": "preprocessing_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Quick model validation to ensure preprocessing quality\n",
        "print(\"\\n8. Quick Model Validation:\")\n",
        "\n",
        "# Split data for quick validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"Training set target distribution: {y_train.value_counts().sort_index().to_dict()}\")\n",
        "print(f\"Test set target distribution: {y_test.value_counts().sort_index().to_dict()}\")\n",
        "\n",
        "# Train a Random Forest model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nQuick validation accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "target_names = ['No Diabetes', 'Diabetes']\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Feature importance analysis\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_scaled.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 5 Most Important Features:\")\n",
        "for idx, row in feature_importance.head().iterrows():\n",
        "    print(f\"  {row['feature']}: {row['importance']:.4f}\")"
      ],
      "metadata": {
        "id": "preprocessing_validation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix Visualization\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "           xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Diabetes Prediction')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance.head(8), x='importance', y='feature', palette='viridis')\n",
        "plt.title('Top 8 Feature Importance - Diabetes Prediction')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "validation_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DIABETES DATASET PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Author: alpian_khairi_C1BO\")\n",
        "print(f\"Dataset: Diabetes Prediction Dataset\")\n",
        "print(f\"\\n📊 DATA OVERVIEW:\")\n",
        "print(f\"✓ Original dataset shape: {df.shape}\")\n",
        "print(f\"✓ Final preprocessed shape: {processed_df.shape}\")\n",
        "print(f\"✓ Features: {len(processed_df.columns)-1} ({len(numerical_features)} numerical, {len(categorical_features)} categorical)\")\n",
        "print(f\"✓ Target: {target_column} (Binary: 0=No Diabetes, 1=Diabetes)\")\n",
        "\n",
        "print(f\"\\n🔧 PREPROCESSING STEPS:\")\n",
        "print(f\"✓ Missing values handled: {df.isnull().sum().sum()} → 0\")\n",
        "print(f\"✓ Duplicates removed: {duplicates}\")\n",
        "print(f\"✓ Data type validation: ✓\")\n",
        "print(f\"✓ Outlier detection completed: ✓\")\n",
        "print(f\"✓ Categorical encoding (Label Encoding): {len(categorical_features)} features\")\n",
        "print(f\"✓ Numerical scaling (StandardScaler): {len(numerical_features)} features\")\n",
        "\n",
        "print(f\"\\n🎯 VALIDATION RESULTS:\")\n",
        "print(f\"✓ Model validation accuracy: {accuracy:.4f}\")\n",
        "print(f\"✓ Most important feature: {feature_importance.iloc[0]['feature']}\")\n",
        "print(f\"✓ Diabetes prevalence: {processed_df[target_column].value_counts()[1]/len(processed_df)*100:.2f}%\")\n",
        "\n",
        "print(f\"\\n💾 OUTPUT FILES:\")\n",
        "print(f\"✓ diabetes_raw.csv (original data backup)\")\n",
        "print(f\"✓ diabetes_preprocessed.csv (ready for ML modeling)\")\n",
        "\n",
        "print(f\"\\n🚀 NEXT STEPS:\")\n",
        "print(f\"• The dataset is now ready for machine learning modeling\")\n",
        "print(f\"• Consider trying different algorithms (SVM, XGBoost, Neural Networks)\")\n",
        "print(f\"• Perform hyperparameter tuning for better performance\")\n",
        "print(f\"• Consider feature selection techniques\")\n",
        "print(f\"• Apply cross-validation for robust model evaluation\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PREPROCESSING COMPLETED SUCCESSFULLY! 🎉\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "final_summary"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}