{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Perkenalan Dataset**\n",
        "\n",
        "Dataset yang digunakan: **Diabetes Prediction Dataset**\n",
        "- Sumber: Healthcare/Medical Data\n",
        "- Jumlah sampel: Variable (depends on dataset size)\n",
        "- Jumlah fitur: 8 (gender, age, hypertension, heart_disease, smoking_history, bmi, HbA1c_level, blood_glucose_level)\n",
        "- Target: diabetes (0 = No Diabetes, 1 = Diabetes)\n",
        "- Tipe data: Mixed (numerik untuk sebagian besar fitur, kategorikal untuk gender dan smoking_history)"
      ],
      "metadata": {
        "id": "kZLRMFl0JyyQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Import Library**"
      ],
      "metadata": {
        "id": "fKADPWcFKlj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Utilities\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "BlmvjLY9M4Yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Memuat Dataset**"
      ],
      "metadata": {
        "id": "f3YIEnAFKrKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Diabetes dataset\n",
        "df = pd.read_csv('diabetes_prediction_dataset.csv')\n",
        "\n",
        "# Define feature categories\n",
        "categorical_features = ['gender', 'smoking_history']\n",
        "numerical_features = ['age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
        "target_column = 'diabetes'\n",
        "\n",
        "# Save raw data\n",
        "df.to_csv('diabetes_raw.csv', index=False)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nDataset columns: {list(df.columns)}\")\n",
        "print(f\"\\nDataset info:\")\n",
        "print(df.info())\n",
        "print(f\"\\nFirst 5 rows:\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "GHCGNTyrM5fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "bgZkbJLpK9UR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic statistics\n",
        "print(\"=== BASIC STATISTICS ===\")\n",
        "print(\"\\nNumerical Features:\")\n",
        "print(df[numerical_features].describe())\n",
        "\n",
        "print(\"\\n=== MISSING VALUES ===\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "print(f\"Total missing values: {missing_values.sum()}\")\n",
        "\n",
        "print(\"\\n=== TARGET DISTRIBUTION ===\")\n",
        "target_dist = df[target_column].value_counts()\n",
        "print(target_dist)\n",
        "print(f\"Diabetes prevalence: {target_dist[1]/len(df)*100:.2f}%\")\n",
        "\n",
        "print(\"\\n=== CATEGORICAL FEATURES DISTRIBUTION ===\")\n",
        "for col in categorical_features:\n",
        "    print(f\"\\n{col}:\")\n",
        "    print(df[col].value_counts())"
      ],
      "metadata": {
        "id": "dKeejtvxM6X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comprehensive visualizations\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# 1. Target distribution\n",
        "target_counts = df[target_column].value_counts()\n",
        "axes[0].pie(target_counts.values, labels=['No Diabetes', 'Diabetes'], autopct='%1.1f%%', \n",
        "           colors=['lightblue', 'lightcoral'])\n",
        "axes[0].set_title('Diabetes Distribution')\n",
        "\n",
        "# 2. Age distribution\n",
        "axes[1].hist(df['age'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[1].set_title('Age Distribution')\n",
        "axes[1].set_xlabel('Age')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "# 3. BMI distribution\n",
        "axes[2].hist(df['bmi'], bins=30, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "axes[2].set_title('BMI Distribution')\n",
        "axes[2].set_xlabel('BMI')\n",
        "axes[2].set_ylabel('Frequency')\n",
        "\n",
        "# 4. Gender distribution\n",
        "gender_counts = df['gender'].value_counts()\n",
        "axes[3].bar(gender_counts.index, gender_counts.values, color=['pink', 'lightblue'])\n",
        "axes[3].set_title('Gender Distribution')\n",
        "axes[3].set_xlabel('Gender')\n",
        "axes[3].set_ylabel('Count')\n",
        "axes[3].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# 5. Smoking history distribution\n",
        "smoking_counts = df['smoking_history'].value_counts()\n",
        "axes[4].bar(range(len(smoking_counts)), smoking_counts.values, color='orange', alpha=0.7)\n",
        "axes[4].set_xticks(range(len(smoking_counts)))\n",
        "axes[4].set_xticklabels(smoking_counts.index, rotation=45, ha='right')\n",
        "axes[4].set_title('Smoking History Distribution')\n",
        "axes[4].set_ylabel('Count')\n",
        "\n",
        "# 6. HbA1c levels\n",
        "axes[5].hist(df['HbA1c_level'], bins=20, alpha=0.7, color='purple', edgecolor='black')\n",
        "axes[5].set_title('HbA1c Level Distribution')\n",
        "axes[5].set_xlabel('HbA1c Level')\n",
        "axes[5].set_ylabel('Frequency')\n",
        "\n",
        "# 7. Blood glucose levels\n",
        "axes[6].hist(df['blood_glucose_level'], bins=30, alpha=0.7, color='red', edgecolor='black')\n",
        "axes[6].set_title('Blood Glucose Level Distribution')\n",
        "axes[6].set_xlabel('Blood Glucose Level')\n",
        "axes[6].set_ylabel('Frequency')\n",
        "\n",
        "# 8. Hypertension vs Diabetes\n",
        "hyp_diabetes = pd.crosstab(df['hypertension'], df[target_column])\n",
        "hyp_diabetes.plot(kind='bar', ax=axes[7], color=['lightblue', 'lightcoral'])\n",
        "axes[7].set_title('Hypertension vs Diabetes')\n",
        "axes[7].set_xlabel('Hypertension (0=No, 1=Yes)')\n",
        "axes[7].set_ylabel('Count')\n",
        "axes[7].legend(['No Diabetes', 'Diabetes'])\n",
        "axes[7].tick_params(axis='x', rotation=0)\n",
        "\n",
        "# 9. Heart disease vs Diabetes\n",
        "heart_diabetes = pd.crosstab(df['heart_disease'], df[target_column])\n",
        "heart_diabetes.plot(kind='bar', ax=axes[8], color=['lightgreen', 'orange'])\n",
        "axes[8].set_title('Heart Disease vs Diabetes')\n",
        "axes[8].set_xlabel('Heart Disease (0=No, 1=Yes)')\n",
        "axes[8].set_ylabel('Count')\n",
        "axes[8].legend(['No Diabetes', 'Diabetes'])\n",
        "axes[8].tick_params(axis='x', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDA_plots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation analysis\n",
        "plt.figure(figsize=(12, 8))\n",
        "all_numerical = numerical_features + [target_column]\n",
        "correlation_matrix = df[all_numerical].corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
        "           square=True, linewidths=0.5)\n",
        "plt.title('Correlation Matrix - Numerical Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDA_correlation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature analysis by diabetes status\n",
        "print(\"=== FEATURE STATISTICS BY DIABETES STATUS ===\")\n",
        "for feature in numerical_features:\n",
        "    print(f\"\\n{feature.upper()}:\")\n",
        "    no_diabetes = df[df[target_column] == 0][feature]\n",
        "    diabetes = df[df[target_column] == 1][feature]\n",
        "    \n",
        "    print(f\"No Diabetes - Mean: {no_diabetes.mean():.2f}, Std: {no_diabetes.std():.2f}\")\n",
        "    print(f\"Diabetes - Mean: {diabetes.mean():.2f}, Std: {diabetes.std():.2f}\")\n",
        "    print(f\"Difference: {diabetes.mean() - no_diabetes.mean():.2f}\")"
      ],
      "metadata": {
        "id": "EDA_stats"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Box plots for numerical features by diabetes status\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(numerical_features):\n",
        "    sns.boxplot(data=df, x=target_column, y=feature, ax=axes[i])\n",
        "    axes[i].set_title(f'{feature} by Diabetes Status')\n",
        "    axes[i].set_xlabel('Diabetes (0=No, 1=Yes)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDA_boxplots"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Data Preprocessing**"
      ],
      "metadata": {
        "id": "cpgHfgnSK3ip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== DATA PREPROCESSING STEPS ===\")\n",
        "\n",
        "# 1. Check for missing values\n",
        "print(\"\\n1. Checking for missing values:\")\n",
        "missing_values = df.isnull().sum()\n",
        "print(missing_values)\n",
        "print(f\"Total missing values: {missing_values.sum()}\")\n",
        "\n",
        "# Handle missing values if any\n",
        "df_clean = df.copy()\n",
        "if missing_values.sum() > 0:\n",
        "    print(\"Handling missing values...\")\n",
        "    # Fill numerical missing values with median\n",
        "    for col in numerical_features:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            median_val = df_clean[col].median()\n",
        "            df_clean[col].fillna(median_val, inplace=True)\n",
        "            print(f\"  - Filled {col} with median: {median_val:.2f}\")\n",
        "    \n",
        "    # Fill categorical missing values with mode\n",
        "    for col in categorical_features:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            mode_val = df_clean[col].mode()[0]\n",
        "            df_clean[col].fillna(mode_val, inplace=True)\n",
        "            print(f\"  - Filled {col} with mode: {mode_val}\")\n",
        "else:\n",
        "    print(\"No missing values found.\")\n",
        "\n",
        "# 2. Check for duplicates\n",
        "print(\"\\n2. Checking for duplicates:\")\n",
        "duplicates = df_clean.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "\n",
        "if duplicates > 0:\n",
        "    print(\"Removing duplicates...\")\n",
        "    df_clean = df_clean.drop_duplicates().reset_index(drop=True)\n",
        "    print(f\"Shape after removing duplicates: {df_clean.shape}\")\n",
        "else:\n",
        "    print(\"No duplicates found.\")"
      ],
      "metadata": {
        "id": "preprocessing_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Data type cleaning and validation\n",
        "print(\"\\n3. Data type cleaning and validation:\")\n",
        "\n",
        "# Clean categorical features\n",
        "for col in categorical_features:\n",
        "    df_clean[col] = df_clean[col].astype(str).str.strip().str.lower()\n",
        "    print(f\"Cleaned {col}: {df_clean[col].unique()}\")\n",
        "\n",
        "# Validate numerical ranges\n",
        "print(\"\\nValidating numerical ranges:\")\n",
        "original_len = len(df_clean)\n",
        "\n",
        "# Age validation (0-120)\n",
        "if 'age' in df_clean.columns:\n",
        "    df_clean = df_clean[(df_clean['age'] > 0) & (df_clean['age'] <= 120)]\n",
        "    print(f\"Age validation: {original_len} â†’ {len(df_clean)} rows\")\n",
        "\n",
        "# BMI validation (10-70)\n",
        "if 'bmi' in df_clean.columns:\n",
        "    df_clean = df_clean[(df_clean['bmi'] > 0) & (df_clean['bmi'] <= 70)]\n",
        "    print(f\"BMI validation: kept rows with BMI > 0 and <= 70\")\n",
        "\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "print(f\"Final shape after validation: {df_clean.shape}\")"
      ],
      "metadata": {
        "id": "preprocessing_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Outlier detection using IQR method\n",
        "print(\"\\n4. Outlier Detection using IQR method:\")\n",
        "\n",
        "def detect_outliers_iqr(data, column):\n",
        "    Q1 = data[column].quantile(0.25)\n",
        "    Q3 = data[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
        "    return outliers.index, lower_bound, upper_bound\n",
        "\n",
        "outlier_summary = {}\n",
        "for feature in numerical_features:\n",
        "    outlier_indices, lower, upper = detect_outliers_iqr(df_clean, feature)\n",
        "    outlier_count = len(outlier_indices)\n",
        "    outlier_percentage = outlier_count/len(df_clean)*100\n",
        "    \n",
        "    outlier_summary[feature] = {\n",
        "        'count': outlier_count,\n",
        "        'percentage': outlier_percentage,\n",
        "        'bounds': (lower, upper)\n",
        "    }\n",
        "    \n",
        "    print(f\"{feature}: {outlier_count} outliers ({outlier_percentage:.2f}%) - Bounds: [{lower:.2f}, {upper:.2f}]\")\n",
        "\n",
        "print(\"\\nNote: Keeping outliers as they may be important for diabetes prediction.\")"
      ],
      "metadata": {
        "id": "preprocessing_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Feature encoding\n",
        "print(\"\\n5. Feature Encoding:\")\n",
        "\n",
        "# Separate features and target\n",
        "X = df_clean.drop(target_column, axis=1)\n",
        "y = df_clean[target_column]\n",
        "\n",
        "# Encode categorical features using Label Encoding\n",
        "label_encoders = {}\n",
        "X_encoded = X.copy()\n",
        "\n",
        "for col in categorical_features:\n",
        "    if col in X_encoded.columns:\n",
        "        label_encoders[col] = LabelEncoder()\n",
        "        X_encoded[col] = label_encoders[col].fit_transform(X_encoded[col])\n",
        "        \n",
        "        # Show encoding mapping\n",
        "        mapping = dict(zip(label_encoders[col].classes_, \n",
        "                          label_encoders[col].transform(label_encoders[col].classes_)))\n",
        "        print(f\"Encoded {col}: {mapping}\")\n",
        "\n",
        "print(f\"\\nFeatures after encoding: {list(X_encoded.columns)}\")"
      ],
      "metadata": {
        "id": "preprocessing_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Feature scaling\n",
        "print(\"\\n6. Feature Scaling:\")\n",
        "\n",
        "print(f\"Original ranges for numerical features:\")\n",
        "for feature in numerical_features:\n",
        "    if feature in X_encoded.columns:\n",
        "        print(f\"  {feature}: {X_encoded[feature].min():.2f} - {X_encoded[feature].max():.2f}\")\n",
        "\n",
        "# Apply StandardScaler to numerical features only\n",
        "scaler = StandardScaler()\n",
        "X_scaled = X_encoded.copy()\n",
        "\n",
        "# Scale only numerical features\n",
        "numerical_cols_to_scale = [col for col in numerical_features if col in X_scaled.columns]\n",
        "X_scaled[numerical_cols_to_scale] = scaler.fit_transform(X_encoded[numerical_cols_to_scale])\n",
        "\n",
        "print(f\"\\nAfter scaling - ranges for numerical features:\")\n",
        "for feature in numerical_cols_to_scale:\n",
        "    print(f\"  {feature}: {X_scaled[feature].min():.2f} - {X_scaled[feature].max():.2f}\")\n",
        "\n",
        "print(f\"\\nCategorical features (not scaled): {[col for col in categorical_features if col in X_scaled.columns]}\")"
      ],
      "metadata": {
        "id": "preprocessing_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Create final preprocessed dataset\n",
        "print(\"\\n7. Creating Final Preprocessed Dataset:\")\n",
        "\n",
        "# Combine scaled features with target\n",
        "processed_df = X_scaled.copy()\n",
        "processed_df[target_column] = y\n",
        "\n",
        "print(f\"Final preprocessed dataset shape: {processed_df.shape}\")\n",
        "print(f\"Features: {list(processed_df.columns[:-1])}\")\n",
        "print(f\"Target column: {target_column}\")\n",
        "\n",
        "print(f\"\\nTarget distribution in preprocessed data:\")\n",
        "target_dist = processed_df[target_column].value_counts().sort_index()\n",
        "print(target_dist)\n",
        "print(f\"Diabetes prevalence: {target_dist[1]/len(processed_df)*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nFirst 5 rows of preprocessed data:\")\n",
        "print(processed_df.head())\n",
        "\n",
        "# Save preprocessed data\n",
        "processed_df.to_csv('diabetes_preprocessed.csv', index=False)\n",
        "print(\"\\nPreprocessed data saved as 'diabetes_preprocessed.csv'\")\n",
        "\n",
        "# Verification\n",
        "verification_df = pd.read_csv('diabetes_preprocessed.csv')\n",
        "print(f\"Verification - loaded file shape: {verification_df.shape}\")"
      ],
      "metadata": {
        "id": "preprocessing_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Quick model validation to ensure preprocessing quality\n",
        "print(\"\\n8. Quick Model Validation:\")\n",
        "\n",
        "# Split data for quick validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set shape: {X_train.shape}\")\n",
        "print(f\"Test set shape: {X_test.shape}\")\n",
        "print(f\"Training set target distribution: {y_train.value_counts().sort_index().to_dict()}\")\n",
        "print(f\"Test set target distribution: {y_test.value_counts().sort_index().to_dict()}\")\n",
        "\n",
        "# Train a Random Forest model\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nQuick validation accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report\n",
        "target_names = ['No Diabetes', 'Diabetes']\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "\n",
        "# Feature importance analysis\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_scaled.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(f\"\\nTop 5 Most Important Features:\")\n",
        "for idx, row in feature_importance.head().iterrows():\n",
        "    print(f\"  {row['feature']}: {row['importance']:.4f}\")"
      ],
      "metadata": {
        "id": "preprocessing_validation"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix Visualization\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "           xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title('Confusion Matrix - Diabetes Prediction')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# Feature importance visualization\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(data=feature_importance.head(8), x='importance', y='feature', palette='viridis')\n",
        "plt.title('Top 8 Feature Importance - Diabetes Prediction')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "validation_viz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"DIABETES DATASET PREPROCESSING SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Author: alpian_khairi_C1BO\")\n",
        "print(f\"Dataset: Diabetes Prediction Dataset\")\n",
        "print(f\"\\nðŸ“Š DATA OVERVIEW:\")\n",
        "print(f\"âœ“ Original dataset shape: {df.shape}\")\n",
        "print(f\"âœ“ Final preprocessed shape: {processed_df.shape}\")\n",
        "print(f\"âœ“ Features: {len(processed_df.columns)-1} ({len(numerical_features)} numerical, {len(categorical_features)} categorical)\")\n",
        "print(f\"âœ“ Target: {target_column} (Binary: 0=No Diabetes, 1=Diabetes)\")\n",
        "\n",
        "print(f\"\\nðŸ”§ PREPROCESSING STEPS:\")\n",
        "print(f\"âœ“ Missing values handled: {df.isnull().sum().sum()} â†’ 0\")\n",
        "print(f\"âœ“ Duplicates removed: {duplicates}\")\n",
        "print(f\"âœ“ Data type validation: âœ“\")\n",
        "print(f\"âœ“ Outlier detection completed: âœ“\")\n",
        "print(f\"âœ“ Categorical encoding (Label Encoding): {len(categorical_features)} features\")\n",
        "print(f\"âœ“ Numerical scaling (StandardScaler): {len(numerical_features)} features\")\n",
        "\n",
        "print(f\"\\nðŸŽ¯ VALIDATION RESULTS:\")\n",
        "print(f\"âœ“ Model validation accuracy: {accuracy:.4f}\")\n",
        "print(f\"âœ“ Most important feature: {feature_importance.iloc[0]['feature']}\")\n",
        "print(f\"âœ“ Diabetes prevalence: {processed_df[target_column].value_counts()[1]/len(processed_df)*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ OUTPUT FILES:\")\n",
        "print(f\"âœ“ diabetes_raw.csv (original data backup)\")\n",
        "print(f\"âœ“ diabetes_preprocessed.csv (ready for ML modeling)\")\n",
        "\n",
        "print(f\"\\nðŸš€ NEXT STEPS:\")\n",
        "print(f\"â€¢ The dataset is now ready for machine learning modeling\")\n",
        "print(f\"â€¢ Consider trying different algorithms (SVM, XGBoost, Neural Networks)\")\n",
        "print(f\"â€¢ Perform hyperparameter tuning for better performance\")\n",
        "print(f\"â€¢ Consider feature selection techniques\")\n",
        "print(f\"â€¢ Apply cross-validation for robust model evaluation\")\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PREPROCESSING COMPLETED SUCCESSFULLY! ðŸŽ‰\")\n",
        "print(\"=\"*70)"
      ],
      "metadata": {
        "id": "final_summary"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}